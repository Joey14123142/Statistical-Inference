svm.pred[1]
svm.pred[[1]]
unlist(svm.pred)
length(svm.pred)
svm <- prediction(svm.pred[[1:278]],cv.test$Outcome)
as.factor(svm.pred)
levels(svm.pred)
list(svm.pred)
as.data.frame(svm.pred)
as.data.frame(svm.pred)[2]
as.data.frame(svm.pred)[1]
as.data.frame(svm.pred)[[1]]
svm <- prediction(as.data.frame(svm.pred)[[1]],cv.test$Outcome)
svm <- prediction(svm.pred,cv.test$Outcome)
test$Outcome
rf.pred
head(rf.pred[,2])
svm.pred$predictions
svm <- prediction(svm.pred,cv.test$Outcome)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151]
decision.values = TRUE,
probability = TRUE)
svm.fit <- svm(x=cv.train[1:151],y= cv.train$Outcome, kernel = "linear", cost = 1, scale = FALSE,
probability = TRUE)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151]
decision.values = TRUE,
probability = TRUE)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE)
head(svm.pred)
svm <- prediction(svm.pred,cv.test$Outcome)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE, decision.values = TRUE)
svm.pred
svm <- prediction(svm.pred[,2],cv.test$Outcome)
head(svm.pred)
class(svm.pred)
head(svm.pred)
probs <-  attr(svm.pred, "probabilities")[,1]
svm <- prediction(probs,cv.test$Outcome)
err.vect[i] <- performance(svm, "auc")@y.values[[1]]
print(paste("AUC for the fold",i,":",err.vect[i]))
k=10
n= floor(nrow(training)/k)
err.vect = rep(NA,k)
for (i in 1:k){
s1 <- ((i-1)* n+1)
s2 <- (i*n)
subset <- s1:s2
cv.train <- training[-subset,]
cv.test <- training[subset,]
svm.fit <- svm(x=cv.train[1:151],y= cv.train$Outcome, kernel = "linear", cost = 1, scale = FALSE,
probability = TRUE)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE, decision.values = TRUE)
probs <-  attr(svm.pred, "probabilities")[,1]
svm <- prediction(probs,cv.test$Outcome)
err.vect[i] <- performance(svm, "auc")@y.values[[1]]
print(paste("AUC for the fold",i,":",err.vect[i]))
}
print(paste("Average AUC:",mean(err.vect)))
for (i in 1:k){
s1 <- ((i-1)* n+1)
s2 <- (i*n)
subset <- s1:s2
cv.train <- training[-subset,]
cv.test <- training[subset,]
svm.fit <- svm(x=cv.train[1:151],y= cv.train$Outcome, kernel = "linear", cost = 1, scale = FALSE,
probability = TRUE)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE, decision.values = TRUE)
probs <-  attr(svm.pred, "probabilities")[,2]
svm <- prediction(probs,cv.test$Outcome)
err.vect[i] <- performance(svm, "auc")@y.values[[1]]
print(paste("AUC for the fold",i,":",err.vect[i]))
}
k <- 10
n= floor(nrow(training)/k)
err.rf<- rep(NA,k)
err.svm <- rep(NA, k)
for (i in 1:k){
s1 <- ((i-1)* n+1)
s2 <- (i*n)
subset <- s1:s2
cv.train <- training[-subset,]
cv.test <- training[subset,]
rf.fit <- randomForest(x=cv.train[1:151],y= cv.train$Outcome)
rf.pred <- predict (rf.fit,newdata= cv.test[1:151],type<-"prob")
rf<-prediction(rf.pred[,2],cv.test$Outcome)
err.rf[i]<-performance(rf,"auc")@y.values[[1]]
print(paste("AUC for the fold",i,":",err.rf[i]))
svm.fit <- svm(x=cv.train[1:151],y= cv.train$Outcome, kernel = "linear", cost = 1, scale = FALSE,
probability = TRUE)
svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE, decision.values = TRUE)
probs <-  attr(svm.pred, "probabilities")[,2]
svm <- prediction(probs,cv.test$Outcome)
err.svm[i] <- performance(svm, "auc")@y.values[[1]]
print(paste("AUC for the fold",i,":",err.svm[i]))
}
source('~/.active-rstudio-document', echo=TRUE)
set.seed(1)
costs <- tune(svm, Outcome~., data = training, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10)),
scale = FALSE)
summary(costs)
bestsvm <- costs$best.model
summary(bestsvm)
svm.pred <- predict(bestsvm, test)
table(Prediction = svm.pred, Truth = test$Outcome)
summari = function(predict_label, actual_label, true_value, false_value) {
tp <- sum((predict_label == true_value) * (actual_label == true_value))
fp <- sum((predict_label == true_value) * (actual_label == false_value))
tn <- sum((predict_label == false_value) * (actual_label == false_value))
fn <- sum((predict_label == false_value) * (actual_label == true_value))
total <- tp + fp + tn + fn
accuracy <- (tp + tn) / total
sensitivity <- tp / (tp + fn)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
F1 <- 2*((precision*sensitivity)/(precision+sensitivity))
confusion_matrix <- matrix(c(tp, fp, fn, tn), 2, 2)
colnames(confusion_matrix) <- c("Predicted True", "Predicted False")
rownames(confusion_matrix) <- c("Actual True", "Actual False")
return(list(true_positive = tp,
false_positive = fp,
true_negative = tn,
false_negative = fn,
total = total,
confusion_matrix = confusion_matrix,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
precision = precision,
F1score = F1 ))
}
sv <- summari(svm.pred, test$Outcome, 'mutagen','nonmutagen')
sv
data(AquaticTox)
start <- function(pkg){
npkg <- pkg[!(pkg %in% installed.packages()[,"Package"])]
if (length(npkg))
install.packages(npkg, dependencies = TRUE)
lapply(pkg, require, character.only=TRUE)
}
pkgs <- c("QSARdata",'caret','ggplot2','pls')
start(pkgs)
data(AquaticTox)
head(AquaticTox_Outcome)
descriptors<-AquaticTox_QuickProp
data<-cbind(descriptors,AquaticTox_Outcome$Activity)
head(data)
?AquaticTox
start <- function(pkg){
npkg <- pkg[!(pkg %in% installed.packages()[,"Package"])]
if (length(npkg))
install.packages(npkg, dependencies = TRUE)
lapply(pkg, require, character.only=TRUE)
}
pkgs <- c("ROCR","rcdk","randomForest","MASS","e1071")
start(pkgs)
# Parse smiles structures from the prefiltered dataset --------------------
dat1<-read.csv("mutagendata.smi",sep="\t",header=F) # smile + IDs + Outcomec
# sum(dat1[,3]=="mutagen")/nrow(dat1) # 0.59
smi <-lapply(as.character(dat1$V1),parse.smiles)
cmp.fp<-vector("list",nrow(dat1))
# Generate and save fingerprints in dataframe ---------------------------------------------------
for (i in 1:nrow(dat1)){
cmp.fp[i]<-lapply(smi[[i]][1],get.fingerprint,type="maccs")
}
fpmac<-fp.to.matrix(cmp.fp)
# colSums(fpmac)
# fingerprints with the sum of 0 are removed
cmp.finger<-as.data.frame(fpmac[,colSums(fpmac) != 0])
dataset<-cbind(cmp.finger,dat1$V3)
colnames(dataset)[152]<-"Outcome"
# Split dataset on 8:2 into training and test sets ------------------------
mask <- sample(2, nrow(dataset), replace = TRUE, prob=c(0.8,0.2))
training <- dataset[mask == 1,]
test <- dataset[mask == 2,]
totest <- expand.grid(mtry=4:7,nodesize=3:6)
la <- lapply(1:nrow(totest),function(ii) {
ee <-    errorest(Outcome ~.,
mtry=totest$mtry[ii],
nodesize=totest$nodesize[ii],
model=randomForest,
data=training)
cc <- c(mtry=totest$mtry[ii],nodesize=totest$nodesize[ii],error=ee$error)
print(cc)
cc
})
sla <- do.call(rbind,la)
sla <- as.data.frame(sla)
xyplot(error ~ mtry, groups= nodesize, data=sla,auto.key=TRUE,type=’l’)
library(lattice)
library(ipred)
totest <- expand.grid(mtry=4:7,nodesize=3:6)
la <- lapply(1:nrow(totest),function(ii) {
ee <-    errorest(Outcome ~.,
mtry=totest$mtry[ii],
nodesize=totest$nodesize[ii],
model=randomForest,
data=training)
cc <- c(mtry=totest$mtry[ii],nodesize=totest$nodesize[ii],error=ee$error)
print(cc)
cc
})
cuts <- seq(.45,.65,.001)
# model evaluation 1b
eval2 <- expand.grid(nodesize=seq(4,100,8),mtry=seq(2,8,2),count=1:10)
sach <- lapply( 1:nrow(eval2),function(i) {
rfx <- randomForest(Outcome ~.,
data=training,
nodesize=eval2$nodesize[i],
mtry=eval2$mtry[i],
ntree=1000)
pp <- predict(rfx,type="prob")
nerr=sapply(cuts,function(cc){
decide=factor(as.numeric(pp[,1]<cc))
sum(decide==training$Outcome)})
data.frame(
nerr=nerr,
cuts=cuts,
mtry=eval2$mtry[i],
nodesize=eval2$nodesize[i],
i=rep(i,length(cuts)))
})
sach <- do.call(rbind,sach)
xyplot(nerr ~ cuts | nodesize + mtry ,group=i, data=sach,auto.key=FALSE,type="l")
sach <- lapply( 1:nrow(eval2),function(i) {
rfx <- randomForest(Outcome ~.,
data=training,
nodesize=eval2$nodesize[i],
mtry=eval2$mtry[i],
ntree=1000)
pp <- predict(rfx,type="prob")
nerr=sapply(cuts,function(cc){
decide=factor(as.numeric(pp[,2]<cc))
sum(decide==training$Outcome)})
data.frame(
nerr=nerr,
cuts=cuts,
mtry=eval2$mtry[i],
nodesize=eval2$nodesize[i],
i=rep(i,length(cuts)))
})
rf <- randomForest(Outcome~., training, mtry = 75, importance=TRUE)
pp <- predict(rf, type = "prob")
head(pp)
class(pp[,2])
decide=factor(as.numeric(pp[,2]<0.5
)
)
head(decide)
nnew <- 10
mnew <- 3
varnew <- 0.6
nold <- 10
mold <- 5
varold <- 0.68
p <- 0.95
pair.var <- ((nnew-1)*varnew + (nold-1)*varold) / (nnew + nold - 2)
ci <- (mnew - mold) + c(-1, 1) * qt(p + (1-p)/2, (nnew+nold-2)) * sqrt(pair.var) * sqrt(1/nnew + 1/nold)
ci
ci2 <- (mnew - mold) + c(-1, 1) * qt(0.9 + (1-0.9)/2, (nnew+nold-2)) * sqrt(pair.var) * sqrt(1/nnew + 1/nold)
ci2
pair.var <- (varnew + varold)/2
ci <- (mnew - mold) + c(-1, 1) * qt(p + (1-p)/2, 2*n-2) * sqrt(pair.var) * sqrt(2/n)
ci
p <- 0.95
ci <- (mnew - mold) + c(-1, 1) * qt(p + (1-p)/2, 2*n-2) * sqrt(pair.var) * sqrt(2/n)
ci
diet.m <- -3
diet.var <- 1.5
place.m <- 1
place.var <- 1.8
n <- 9
pair.var <- (diet.var^2 + place.var^2)/2
ci <- diet.m - place.m +c(-1,1)*qt(p + (1-p)/2, df=(2*n - 2))*sqrt(pair.var)*sqrt(2/n)
ci
mt <- -3
mp <- 1
st <- 1.5
sp <- 1.8
nt <- 9
np <- 9
p <- 0.9
pooled_variance <- ((nt-1)*st^2 + (np-1)*sp^2)/(nt + np - 2)
ci <- mt - mp + c(-1,1) * qt(p + (1-p)/2, df=(nt+np-2)) * sqrt(pooled_variance) * sqrt(1/nt + 1/np)
ci
pooled_variance
pair.var
p <- 0.9
ci <- diet.m - place.m +c(-1,1)*qt(p + (1-p)/2, df=(2*n - 2))*sqrt(pair.var)*sqrt(2/n)
ci
ci <- diet.m - place.m +c(-1,1)*qt(p + (1-p)/2, df=(n+n - 2))*sqrt(pair.var)*sqrt(1/n + 1/n)
ci
ci <- diet.m - place.m +c(-1,1)*qt(p + (1-p)/2, df=(n+n - 2))*sqrt(pair.var)*sqrt(2/n)
ci
pharm <- data.frame(baseline=c(140,138,150,148,135), week2=c(132,135,151,146,130))
t.test(pharm$baseline,pharm$week2, alternative = "two.sided", paired=TRUE)
n = 9
m = 1100
s = 30
a = 0.05
m+c(-1,1)*qt(1-(a/2),n-1)*s/sqrt(n)
binom.test(x = 3, n = 4, p=.5, alternative="greater")
p = 1/100
p_ = 10/1787
n=1787
serror = sqrt(p*(1-p)/n)
test_z = (p-p_)/serror
pnorm(test_z, lower.tail=FALSE)
mean.diff = -3-1
df = (9 + 9 - 2)
m_tr = -3
m_pb = 1
s_tr = 1.5
s_pb = 1.8
pooled.var = (s_tr^2 * 9 + s_pb^2 * 9)/df
se.diff = sqrt(pooled.var/9 + pooled.var/9)
t.obt = mean.diff / se.diff
t.obt
p.value = 2*pt(t.obt, df=df) #two tailed
p.value
power.t.test(n=100, delta = .01, sd = .04, type = "one.sample", alt = "one.sided")$power
power.t.test(power = .9, delta = .01, sd = .04, type="one.sample", alt="one.sided")$n
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(inTrain)
length(inTrain)
library(Hmisc)
?cut2
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
library(gridExtra)
library(dplyr)
?mutate
training <- mutate(training, index=1:nrow(training))
cutIndex <- cut2(training$index, g=10)
breaks <- 10
cutIndex <- cut2(training$index, g=10)
breaks <- 10
for(i in 1:ncol(training)){
x <- training[,i]
return(qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks)))
}
for(i in 1:ncol(training)){
x <- training[,i]
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
for(i in 1:ncol(training)){
x <- training[,i]
q <- qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
return(q)
}
q
par(mfrow=c(8,1))
for(i in 1:ncol(training)){
x <- training[,i]
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
for(i in 1:ncol(training)){
x <- training[,i]
q <- qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
q
for(i in 1:ncol(training)){
x <- training[,i]
par(mfrow=c(8,1))
q <- qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
for(i in 1:ncol(training)){
x <- training[,i]
par(mfrow=c(8,1))
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
for(i in 1:ncol(training)){
x <- training[,i]
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
figure <- par(mfrow=c(8,1))
class(figure)
for(i in 1:ncol(training)){
x <- training[,i]
figure[i] <- qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
}
figure
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
head
head(training)
i <- 3
x <- training[,i]
qplot(index, CompressiveStrength, data = training, color = cut2(x, g=breaks))
hist(training$Superplasticizer)
hist(training$Superplasticizer, breaks = 20)
hist(training$Superplasticizer, breaks = 20)
hist(training$Superplasticizer, breaks=20)
par(mfrow=c(1,1))
hist(training$Superplasticizer, breaks = 20)
hist(log(training$Superplasticizer+1), breaks = 20)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
IL_col_idx
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.9)
preObj
subTrain <- training[, IL_col_idx]
train <- training[, IL_col_idx]
train$diagnosis <- training$diagnosis
test <- testing[ ,IL_col_idx]
test$diagnosis <- testing$diagnosis
nonPCA <- train(diagnosis~., data = subtrain, method="glm")
subtrain <- training[, IL_col_idx]
subtrain$diagnosis <- training$diagnosis
test <- testing[ ,IL_col_idx]
test$diagnosis <- testing$diagnosis
nonPCA <- train(diagnosis~., data = subtrain, method="glm")
dim(test)
PCA <- train(diagnosis~., data = subtrain,
preProcess(subtrain[,-13], method = c("pca"), thresh = 0.8),
method = "glm")
PCA <- train(diagnosis~., data = subtrain,
preProcess(subtrain[,-13], method = "pca", thresh = 0.8),
method = "glm")
PCA <- train(diagnosis~., data = subtrain,
preProcess(subtrain[,-13], method = c("center","scale","pca"), thresh = 0.8),
method = "glm")
PCA <- train(diagnosis~., data = subtrain,
preProcess = c("center","scale","pca"), thresh = 0.8),
method = "glm")
PCA <- train(diagnosis~., data = subtrain,
preProcess = c("center","scale","pca"),
thresh = 0.8,
method = "glm")
PCA <- train(diagnosis~., data = subtrain,
preProcess = c("center","scale","pca"),
method = "glm")
subtrain <- training[, IL_col_idx]
subtrain$diagnosis <- training$diagnosis
test <- testing[ ,IL_col_idx]
test$diagnosis <- testing$diagnosis
nonPCA <- train(diagnosis~., data = subtrain, method="glm")
nonPCA.pred <- confusionMatrix(test[,13],
predict(nonPCA, test[,-13]))
pca.pre <- preProcess(new_training[, -13], method=c('center', 'scale', 'pca'), thresh=0.8)
pca.train <- predict(pca.pre, subtrain[, -13])
pca.test <- predict(pca.pre, test[, -13])
pca <- train(diagnosis ~ ., data=pca.train, method="glm")
pca.pred <- confusionMatrix(test[, 13], predict(pca, pca.test))
pca.pre <- preProcess(subtrain[, -13], method=c('center', 'scale', 'pca'), thresh=0.8)
pca.train <- predict(pca.pre, subtrain[, -13])
pca.test <- predict(pca.pre, test[, -13])
pca <- train(diagnosis ~ ., data=pca.train, method="glm")
pca <- train(pca.train$diagnosis ~ ., data=pca.train, method="glm")
pca <- train(diagnosis~., data=pca.train, method="glm")
pca.pred <- confusionMatrix(test[, 13], predict(pca, pca.test))
pca <- train(diagnosis~., data=pca.train[,-13], method="glm")
names(pca.train)
pca <- train(subtrain$diagnosis~., data=pca.train, method="glm")
pca.pred <- confusionMatrix(test[, 13], predict(pca, pca.test))
nonPCA.pred
hist(log(training$Superplasticizer), breaks = 20)
hist(log(training$Superplasticizer+1), breaks = 20)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(log(training$Superplasticizer+1), breaks = 20)
sum(training$Superplasticizer==0)
sum(training$Superplasticizer<0)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
trainIndex = createDataPartition(Case, p = 0.50,list=FALSE)
data
names(data)
data(segmentationOriginal)
data(segmentationOriginal)
names(data)
dt <- segmentationOriginal
names(dt)
trainIndex = createDataPartition(Case, p = 0.50,list=FALSE)
trainIndex = createDataPartition(dt$Case, p = 0.50,list=FALSE)
training = adData[trainIndex,]
training = dt[trainIndex,]
testing = dt[-trainIndex,]
set.seed(125)
